{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 ê³¼ì œ\n",
    "> ì¸ê³µì§€ëŠ¥ ìŠ¤í„°ë”” ì¼ê³± ë²ˆì§¸ ê³¼ì œì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ë°°ìš´ ë‹¤ì–‘í•œ ì§€ì‹ë“¤ì„ ì‹¤ìŠµì„ í†µí•´ì„œ í™œìš©í•´ ë³¼ ì‹œê°„ì„ ê°€ì§ˆ ê²ƒì…ë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Transformer\n",
    "\n",
    "ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ì´ ê³„ì‚°ë˜ëŠ” multi-head attentionì—ì„œ query, key, value ë²¡í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ \n",
    "projection matrix $(â€¯W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}â€‹â€‹â€¯)$ëŠ” head ê°„ì— sharing ëœë‹¤. <br>\n",
    "***\n",
    "$â€¯MultiHead(Q,K,V)=Concat(head_1, \\cdots, head_h)W^{O} $ (ì´ë•Œ, $W^{O}$ ëŠ” Outputì„ ë§Œë“¤ë•Œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬)<br>\n",
    "where $head_i=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$ (ì´ë•Œ, $Q, K, V$ ëŠ” ì…ë ¥ì—ì„œ tokenizeëœ ë‹¨ì–´ë“¤ì˜ ì„ë² ë”© ë²¡í„° $Q = K = V$ )\n",
    "```python\n",
    "(1) ì˜ˆ\n",
    "(2) ì•„ë‹ˆì˜¤\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ˜‰\n",
    "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> Transformer\n",
    "```python\n",
    "Transformer ëª¨ë¸ì—ì„œ ê° ì…ë ¥ í† í°ë“¤ì´ ê°€ì§„ ìˆœì„œë¥¼ ì…ë ¥í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ê³ ë¥´ì‹œì˜¤. \n",
    "\n",
    "(1) Positional Encoding \n",
    "(2) Encoder-Decoder attention \n",
    "(3) Layer normalization \n",
    "(4) Masked decoder self-attention \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ˜‰\n",
    "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> GPT\n",
    "```python\n",
    "GPT-1 ëª¨ë¸ì´ ì–´ë–»ê²Œ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ”ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ˜‰\n",
    "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
    "Transformerì˜ ë””ì½”ë” êµ¬ì¡°ë§Œì„ ì‚¬ìš©, \n",
    "self-attention ë§¤ì»¤ë‹ˆì¦˜ì„ ì´ìš©. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> GPT\n",
    "```python\n",
    "GPT-1 ëª¨ë¸ì˜ \"GPT\" ì•½ìëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?\n",
    "\n",
    "(1) Generalized Pre-trained Transformer\n",
    "(2) Generative Pre-trained Transformer\n",
    "(3) Globalized Pre-processing Transformer\n",
    "(4) Gradient Propagation Technique\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ˜‰\n",
    "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### â“ <font color='red'><b>[ í€´ì¦ˆ ]</b></font> BERT\n",
    "```python\n",
    "ë‹¤ìŒ ì¤‘ BERTì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì˜³ì§€ ì•Šì€ ê²ƒì„ ê³ ë¥´ì‹œì˜¤.\n",
    "\n",
    "(1) í•™ìŠµ ë°ì´í„°ì—ì„œ [MASK] í† í°ì´ ì„ íƒë˜ëŠ” ë¹„ìœ¨ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ì€ ê²½ìš°, ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë¹„ìš©ì´ ì¦ê°€í•œë‹¤. \n",
    "(2) Unidirectional modelë¡œ ìì—°ì–´ ìƒì„±ì— íŠ¹í™”ëœ ëª¨ë¸ì´ë‹¤. \n",
    "(3) ì…ë ¥ ì‹œí€€ìŠ¤ ì¤‘ ì¼ë¶€ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ë§ì¶”ëŠ” masked language modeling (masked LM)ì„ í†µí•´ pre-trainingì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. \n",
    "(4) ì‚¬ì „í•™ìŠµì„ ìœ„í•œ [MASK] í† í°ì€ randomí•˜ê²Œ ì„ íƒëœë‹¤. \n",
    "(5) Unlabeled ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ self-supervised learningì„ ì ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ì´ë‹¤. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ˜‰\n",
    "# TODO : ì •ë‹µì„ ì ì–´ì£¼ì„¸ìš”\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì‹¤ìŠµ ]</b></font> Multi-head Attention\n",
    "```python\n",
    "ì´ë²ˆ ì‹¤ìŠµì„ í†µí•´ ë‹¤ìŒ 2ê°€ì§€ë¥¼ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
    "1. Multi-head attention ë° self-attentionì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "2. ê° ê³¼ì •ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì—°ì‚°ê³¼ input/output í˜•íƒœë¥¼ ì´í•´í•©ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ğŸ™\n",
    "ë¨¼ì € ì½”ë“œ ì‹¤í–‰ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ import í•´ë´…ì‹œë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ì „ì²˜ë¦¬\n",
    "```python\n",
    "ì €ë²ˆ ì£¼ì°¨ì˜ ë°ì´í„°ì™€ ë¹„ìŠ·í•œ í˜•íƒœì…ë‹ˆë‹¤.\n",
    "ë¨¼ì € ì „ì²´ ë‹¨ì–´ ìˆ˜ì¸ vocab_sizeê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.\n",
    "pad_idëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•´ íŒ¨ë”©ì„ ì§„í–‰í•˜ê²Œ ë˜ëŠ”ë° ì´ë•Œ íŒ¨ë”©ì„ ì˜ë¯¸í•˜ëŠ” í† í°ì˜ idì…ë‹ˆë‹¤.\n",
    "sample data ë³´ë©´ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ê²ƒì„ ë³¼ ìˆ˜ ìˆëŠ”ë° ì´ëŠ” ì €í¬ê°€ êµ¬ì„±í•œ vocabì—ì„œ ëª‡ ë²ˆì§¸ ë‹¨ì–´ì¸ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "ë”°ë¼ì„œ ë°ì´í„°ì˜ ê° ìš”ì†Œë¥¼ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì¥ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ì£¼ì–´ì§„ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•œ padding í•¨ìˆ˜ë¥¼ ë„ì…í•©ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ í™•ì¸í•´ ë³´ë©´ ì˜ íŒ¨ë”© ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [77,\n",
       "  65,\n",
       "  51,\n",
       "  77,\n",
       "  19,\n",
       "  15,\n",
       "  35,\n",
       "  19,\n",
       "  23,\n",
       "  97,\n",
       "  50,\n",
       "  46,\n",
       "  53,\n",
       "  42,\n",
       "  45,\n",
       "  91,\n",
       "  66,\n",
       "  3,\n",
       "  43,\n",
       "  10],\n",
       " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
       " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter ì„¸íŒ… ë° embedding\n",
    "```python\n",
    "ìœ„ ë°ì´í„°ë¥¼ ì„ë² ë”©í•˜ì—¬ ì‹¤ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512  # modelì˜ hidden size\n",
    "num_heads = 8  # multi-headì—ì„œì˜ headì˜ ê°œìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: ë°°ì¹˜ ì‚¬ì´ì¦ˆ, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0046, -0.0178,  0.0514,  ..., -0.0254, -1.9394,  0.2982],\n",
      "         [-0.2236, -0.6979,  0.4691,  ...,  0.8568,  1.1622, -0.6842],\n",
      "         [-1.0034, -0.7161,  1.3712,  ..., -0.2181, -0.2377, -0.8508],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[ 0.4331, -0.4980, -0.2755,  ..., -0.1430,  1.2968, -0.5471],\n",
      "         [-0.5339, -0.7122,  0.6966,  ...,  1.4837, -0.7220,  0.1256],\n",
      "         [-0.6730,  0.8889,  0.8030,  ..., -1.9143, -1.0442, -1.6064],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[ 0.1244, -1.8060,  0.3670,  ...,  1.7464, -0.0483, -0.0385],\n",
      "         [ 1.6756,  0.7209, -2.4503,  ...,  0.5764,  0.0857, -1.2608],\n",
      "         [-0.8744,  0.6161, -1.1340,  ..., -0.4147, -0.9417, -0.7027],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3047,  1.6126, -0.7530,  ..., -0.5178, -0.8630,  0.6209],\n",
      "         [ 1.2254, -0.4911,  1.8303,  ..., -0.3099, -0.8174,  1.9783],\n",
      "         [-0.6730,  0.8889,  0.8030,  ..., -1.9143, -1.0442, -1.6064],\n",
      "         ...,\n",
      "         [ 1.3433,  0.3143, -0.5930,  ..., -0.4165, -0.1450,  0.5027],\n",
      "         [-0.4853, -0.1089,  0.5043,  ...,  0.6195, -1.5236, -0.4439],\n",
      "         [-0.6199,  0.2733, -1.3323,  ...,  0.2144, -1.4687,  0.3000]],\n",
      "\n",
      "        [[-1.3036, -1.5289, -0.6917,  ...,  1.3204, -0.3155, -0.0473],\n",
      "         [ 0.9296,  0.0604, -0.7162,  ..., -1.1252, -1.0640, -0.0301],\n",
      "         [ 0.5358, -0.3090, -0.7543,  ...,  1.8795,  0.3861, -1.0702],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[-0.1491,  0.0278, -0.7670,  ..., -0.5967,  0.9102,  0.3555],\n",
      "         [ 0.9296,  0.0604, -0.7162,  ..., -1.1252, -1.0640, -0.0301],\n",
      "         [ 0.4464,  1.2593,  0.8162,  ...,  0.5223,  1.0283, -2.3301],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)\n",
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear transformation & ì—¬ëŸ¬ headë¡œ ë‚˜ëˆ„ê¸°\n",
    "```python\n",
    "Multi-head attention ë‚´ì—ì„œ ì“°ì´ëŠ” linear transformation matrixë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "query, key, valueë¥¼ ì„œë¡œ ë‹¤ë¥¸ linear transformation matrixë¡œ í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤. ë”°ë¼ì„œ ë™ì¼í•œ ë°ì´í„°(batch_emb)ë¡œë¶€í„° ì„œë¡œ ë‹¤ë¥¸ query, key, valueë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "output layerì—ì„œ ì‚¬ìš©ë  í–‰ë ¬ë„ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "q, k, vë¥¼ 'num_head' ê°œì˜ ì°¨ì›ìœ¼ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤. \n",
    "ì‹¤ì œ q, k, v ê°ê°ì˜ ë²¡í„° í¬ê¸°ëŠ” 512ê°€ ì•„ë‹Œ 64ì…ë‹ˆë‹¤. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads # q, k, v ë²¡í„° ì‚¬ì´ì¦ˆ\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "8ê°œì˜ headì— í•„ìš”í•œ q, k, vê°€ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product self-attention êµ¬í˜„\n",
    "```python\n",
    "ê° headì—ì„œ ì‹¤í–‰ë˜ëŠ” self-attention ê³¼ì •ì„ ì‚´í´ë´…ì‹œë‹¤.\n",
    "\n",
    "q, k ë²¡í„°ì˜ ë‚´ì  ì—°ì‚° ì´í›„ì— d_kì˜ ì œê³±ê·¼ìœ¼ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.\n",
    "ì´ëŠ” qì™€ kë¥¼ êµ¬ì„±í•˜ëŠ” ìš”ì†Œì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ë‚´ì ì˜ ê²°ê´ê°’ì— ëŒ€í•´ì„œë„ ìœ ì§€ì‹œì¼œì£¼ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´í›„ ê³„ì‚°ëœ ê° í–‰ì— ëŒ€í•´ì„œ softmax ì—°ì‚°ì„ í†µí•´ì„œ ê° ìš”ì†Œì˜ í•©ì„ 1ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.\n",
    "```\n",
    "- [Scaled Dot-Product Attention ì°¸ê³ ](https://paperswithcode.com/method/scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0629, 0.0237, 0.0601,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0625, 0.0487, 0.0453,  ..., 0.0663, 0.0663, 0.0663],\n",
      "          [0.0517, 0.0301, 0.0620,  ..., 0.0318, 0.0318, 0.0318],\n",
      "          ...,\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933],\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933],\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933]],\n",
      "\n",
      "         [[0.0580, 0.0319, 0.0398,  ..., 0.0736, 0.0736, 0.0736],\n",
      "          [0.0963, 0.0371, 0.1088,  ..., 0.0319, 0.0319, 0.0319],\n",
      "          [0.0523, 0.0234, 0.0327,  ..., 0.0612, 0.0612, 0.0612],\n",
      "          ...,\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363],\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363],\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363]],\n",
      "\n",
      "         [[0.0374, 0.0362, 0.0524,  ..., 0.0588, 0.0588, 0.0588],\n",
      "          [0.0536, 0.0321, 0.0599,  ..., 0.0679, 0.0679, 0.0679],\n",
      "          [0.0631, 0.0438, 0.0243,  ..., 0.0518, 0.0518, 0.0518],\n",
      "          ...,\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644],\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644],\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0488, 0.0509, 0.0478,  ..., 0.0513, 0.0513, 0.0513],\n",
      "          [0.0396, 0.0485, 0.0265,  ..., 0.0579, 0.0579, 0.0579],\n",
      "          [0.0731, 0.0345, 0.0879,  ..., 0.0409, 0.0409, 0.0409],\n",
      "          ...,\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351],\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351],\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351]],\n",
      "\n",
      "         [[0.0205, 0.0379, 0.0399,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0738, 0.0668, 0.0907,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0361, 0.0620, 0.0609,  ..., 0.0434, 0.0434, 0.0434],\n",
      "          ...,\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051],\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051],\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051]],\n",
      "\n",
      "         [[0.0627, 0.0410, 0.0506,  ..., 0.0316, 0.0316, 0.0316],\n",
      "          [0.0694, 0.0395, 0.0463,  ..., 0.0397, 0.0397, 0.0397],\n",
      "          [0.0560, 0.0404, 0.0482,  ..., 0.0494, 0.0494, 0.0494],\n",
      "          ...,\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456]]],\n",
      "\n",
      "\n",
      "        [[[0.0340, 0.0332, 0.0250,  ..., 0.0574, 0.0574, 0.0574],\n",
      "          [0.0513, 0.0525, 0.0365,  ..., 0.0469, 0.0469, 0.0469],\n",
      "          [0.0415, 0.0624, 0.0690,  ..., 0.0488, 0.0488, 0.0488],\n",
      "          ...,\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572]],\n",
      "\n",
      "         [[0.0517, 0.0980, 0.0389,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0375, 0.0275, 0.0217,  ..., 0.0559, 0.0559, 0.0559],\n",
      "          [0.0383, 0.0633, 0.0467,  ..., 0.0524, 0.0524, 0.0524],\n",
      "          ...,\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455]],\n",
      "\n",
      "         [[0.0161, 0.0252, 0.0438,  ..., 0.0567, 0.0567, 0.0567],\n",
      "          [0.0759, 0.0470, 0.0684,  ..., 0.0487, 0.0487, 0.0487],\n",
      "          [0.0322, 0.0915, 0.0625,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          ...,\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0554, 0.0346, 0.0785,  ..., 0.0492, 0.0492, 0.0492],\n",
      "          [0.0289, 0.0522, 0.0288,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0485, 0.0581, 0.0709,  ..., 0.0488, 0.0488, 0.0488],\n",
      "          ...,\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418]],\n",
      "\n",
      "         [[0.0623, 0.0978, 0.0710,  ..., 0.0399, 0.0399, 0.0399],\n",
      "          [0.0659, 0.0554, 0.0182,  ..., 0.0497, 0.0497, 0.0497],\n",
      "          [0.0357, 0.0322, 0.0543,  ..., 0.0540, 0.0540, 0.0540],\n",
      "          ...,\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602],\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602],\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602]],\n",
      "\n",
      "         [[0.0891, 0.0857, 0.0783,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0467, 0.0496, 0.0789,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0303, 0.0442, 0.0380,  ..., 0.0522, 0.0522, 0.0522],\n",
      "          ...,\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489]]],\n",
      "\n",
      "\n",
      "        [[[0.0501, 0.0406, 0.0330,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          [0.0455, 0.0791, 0.0343,  ..., 0.0554, 0.0554, 0.0554],\n",
      "          [0.0574, 0.0697, 0.0507,  ..., 0.0475, 0.0475, 0.0475],\n",
      "          ...,\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704],\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704],\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704]],\n",
      "\n",
      "         [[0.0152, 0.0379, 0.0888,  ..., 0.0533, 0.0533, 0.0533],\n",
      "          [0.0202, 0.0375, 0.0333,  ..., 0.0606, 0.0606, 0.0606],\n",
      "          [0.0370, 0.0579, 0.0524,  ..., 0.0460, 0.0460, 0.0460],\n",
      "          ...,\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418]],\n",
      "\n",
      "         [[0.0367, 0.0411, 0.0360,  ..., 0.0719, 0.0719, 0.0719],\n",
      "          [0.0695, 0.0444, 0.0977,  ..., 0.0432, 0.0432, 0.0432],\n",
      "          [0.0428, 0.0307, 0.0533,  ..., 0.0648, 0.0648, 0.0648],\n",
      "          ...,\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531],\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531],\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0797, 0.0861, 0.0643,  ..., 0.0410, 0.0410, 0.0410],\n",
      "          [0.0481, 0.0766, 0.0404,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0571, 0.0580, 0.0419,  ..., 0.0557, 0.0557, 0.0557],\n",
      "          ...,\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458]],\n",
      "\n",
      "         [[0.0623, 0.0289, 0.0428,  ..., 0.0575, 0.0575, 0.0575],\n",
      "          [0.1106, 0.0613, 0.0390,  ..., 0.0374, 0.0374, 0.0374],\n",
      "          [0.0342, 0.0826, 0.0458,  ..., 0.0451, 0.0451, 0.0451],\n",
      "          ...,\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732],\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732],\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732]],\n",
      "\n",
      "         [[0.1068, 0.0316, 0.0453,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0559, 0.0331, 0.0520,  ..., 0.0485, 0.0485, 0.0485],\n",
      "          [0.0578, 0.0602, 0.0643,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          ...,\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512],\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512],\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0540, 0.0292, 0.0635,  ..., 0.0614, 0.0748, 0.0903],\n",
      "          [0.0387, 0.0318, 0.0448,  ..., 0.0503, 0.0256, 0.0408],\n",
      "          [0.0491, 0.0541, 0.0729,  ..., 0.0268, 0.0444, 0.0708],\n",
      "          ...,\n",
      "          [0.0582, 0.0522, 0.0326,  ..., 0.0538, 0.0440, 0.0509],\n",
      "          [0.0284, 0.0537, 0.0520,  ..., 0.0431, 0.0633, 0.0451],\n",
      "          [0.0441, 0.0635, 0.0324,  ..., 0.0536, 0.0577, 0.0481]],\n",
      "\n",
      "         [[0.0374, 0.0564, 0.0266,  ..., 0.0421, 0.0466, 0.1090],\n",
      "          [0.0520, 0.0423, 0.0377,  ..., 0.0448, 0.0492, 0.0338],\n",
      "          [0.0509, 0.0425, 0.0595,  ..., 0.0770, 0.0455, 0.0954],\n",
      "          ...,\n",
      "          [0.0429, 0.0212, 0.0318,  ..., 0.0383, 0.0559, 0.0555],\n",
      "          [0.0466, 0.0496, 0.0412,  ..., 0.0497, 0.0364, 0.0462],\n",
      "          [0.0474, 0.0542, 0.0637,  ..., 0.0731, 0.0510, 0.0398]],\n",
      "\n",
      "         [[0.0276, 0.0645, 0.0320,  ..., 0.0632, 0.0341, 0.0871],\n",
      "          [0.0777, 0.0481, 0.0545,  ..., 0.0402, 0.0395, 0.0585],\n",
      "          [0.0565, 0.0505, 0.0651,  ..., 0.0322, 0.0609, 0.0208],\n",
      "          ...,\n",
      "          [0.0498, 0.0659, 0.0672,  ..., 0.0299, 0.0586, 0.0997],\n",
      "          [0.0562, 0.0501, 0.0690,  ..., 0.0466, 0.0342, 0.0540],\n",
      "          [0.0252, 0.0628, 0.0279,  ..., 0.0563, 0.0433, 0.0385]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0508, 0.0720, 0.0604,  ..., 0.0444, 0.0448, 0.0553],\n",
      "          [0.0316, 0.0658, 0.0528,  ..., 0.0611, 0.0443, 0.0478],\n",
      "          [0.0593, 0.0343, 0.0771,  ..., 0.0591, 0.0399, 0.0568],\n",
      "          ...,\n",
      "          [0.0600, 0.0447, 0.0638,  ..., 0.0588, 0.0335, 0.0438],\n",
      "          [0.0185, 0.0296, 0.0535,  ..., 0.0467, 0.0475, 0.0717],\n",
      "          [0.0825, 0.0294, 0.0827,  ..., 0.0184, 0.0390, 0.0930]],\n",
      "\n",
      "         [[0.0334, 0.0249, 0.0567,  ..., 0.0545, 0.0429, 0.0687],\n",
      "          [0.0447, 0.0457, 0.0252,  ..., 0.0561, 0.0541, 0.0429],\n",
      "          [0.0408, 0.0446, 0.0624,  ..., 0.0522, 0.0270, 0.0545],\n",
      "          ...,\n",
      "          [0.0473, 0.0511, 0.0536,  ..., 0.0507, 0.0457, 0.0356],\n",
      "          [0.0552, 0.0394, 0.0609,  ..., 0.0383, 0.0917, 0.0337],\n",
      "          [0.0435, 0.0333, 0.0637,  ..., 0.0375, 0.0780, 0.0404]],\n",
      "\n",
      "         [[0.0441, 0.0548, 0.0282,  ..., 0.0886, 0.0459, 0.0957],\n",
      "          [0.0788, 0.0607, 0.0300,  ..., 0.0868, 0.0515, 0.0336],\n",
      "          [0.0385, 0.0325, 0.0361,  ..., 0.0602, 0.0608, 0.0418],\n",
      "          ...,\n",
      "          [0.0405, 0.0602, 0.0286,  ..., 0.0277, 0.0511, 0.1017],\n",
      "          [0.0614, 0.0429, 0.0553,  ..., 0.0204, 0.0496, 0.0608],\n",
      "          [0.0378, 0.0835, 0.0355,  ..., 0.0626, 0.0542, 0.0361]]],\n",
      "\n",
      "\n",
      "        [[[0.0561, 0.0734, 0.0532,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0392, 0.0535, 0.0559,  ..., 0.0392, 0.0392, 0.0392],\n",
      "          [0.0448, 0.0553, 0.0715,  ..., 0.0497, 0.0497, 0.0497],\n",
      "          ...,\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846],\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846],\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846]],\n",
      "\n",
      "         [[0.0490, 0.0404, 0.0389,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0639, 0.0679, 0.0347,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0395, 0.0394, 0.0407,  ..., 0.0613, 0.0613, 0.0613],\n",
      "          ...,\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393]],\n",
      "\n",
      "         [[0.0665, 0.0689, 0.0335,  ..., 0.0276, 0.0276, 0.0276],\n",
      "          [0.0324, 0.0537, 0.0563,  ..., 0.0713, 0.0713, 0.0713],\n",
      "          [0.0433, 0.0589, 0.0424,  ..., 0.0490, 0.0490, 0.0490],\n",
      "          ...,\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0486, 0.0375, 0.0293,  ..., 0.0331, 0.0331, 0.0331],\n",
      "          [0.0474, 0.0506, 0.0850,  ..., 0.0317, 0.0317, 0.0317],\n",
      "          [0.0737, 0.0350, 0.0440,  ..., 0.0568, 0.0568, 0.0568],\n",
      "          ...,\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339],\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339],\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339]],\n",
      "\n",
      "         [[0.0635, 0.0552, 0.0335,  ..., 0.0479, 0.0479, 0.0479],\n",
      "          [0.0727, 0.0647, 0.0462,  ..., 0.0422, 0.0422, 0.0422],\n",
      "          [0.0297, 0.0630, 0.0395,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          ...,\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020],\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020],\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020]],\n",
      "\n",
      "         [[0.0329, 0.0462, 0.0435,  ..., 0.0500, 0.0500, 0.0500],\n",
      "          [0.0495, 0.0337, 0.0394,  ..., 0.0809, 0.0809, 0.0809],\n",
      "          [0.0296, 0.0239, 0.0404,  ..., 0.0298, 0.0298, 0.0298],\n",
      "          ...,\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450]]],\n",
      "\n",
      "\n",
      "        [[[0.0506, 0.0462, 0.0248,  ..., 0.0879, 0.0879, 0.0879],\n",
      "          [0.0429, 0.0557, 0.0568,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0598, 0.0408, 0.0215,  ..., 0.0632, 0.0632, 0.0632],\n",
      "          ...,\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816],\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816],\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816]],\n",
      "\n",
      "         [[0.0441, 0.0474, 0.0594,  ..., 0.0521, 0.0521, 0.0521],\n",
      "          [0.0569, 0.0705, 0.0384,  ..., 0.0504, 0.0504, 0.0504],\n",
      "          [0.0389, 0.0494, 0.0672,  ..., 0.0421, 0.0421, 0.0421],\n",
      "          ...,\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412]],\n",
      "\n",
      "         [[0.0835, 0.0616, 0.0610,  ..., 0.0327, 0.0327, 0.0327],\n",
      "          [0.0404, 0.0468, 0.0411,  ..., 0.0622, 0.0622, 0.0622],\n",
      "          [0.0649, 0.0360, 0.0775,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          ...,\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547],\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547],\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0595, 0.0512, 0.0331,  ..., 0.0511, 0.0511, 0.0511],\n",
      "          [0.0579, 0.0606, 0.0493,  ..., 0.0379, 0.0379, 0.0379],\n",
      "          [0.0464, 0.0409, 0.0469,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          ...,\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408]],\n",
      "\n",
      "         [[0.0203, 0.0627, 0.0445,  ..., 0.0562, 0.0562, 0.0562],\n",
      "          [0.0458, 0.0660, 0.0727,  ..., 0.0430, 0.0430, 0.0430],\n",
      "          [0.0415, 0.0697, 0.0730,  ..., 0.0407, 0.0407, 0.0407],\n",
      "          ...,\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837],\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837],\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837]],\n",
      "\n",
      "         [[0.0542, 0.0644, 0.0629,  ..., 0.0603, 0.0603, 0.0603],\n",
      "          [0.0435, 0.0284, 0.0353,  ..., 0.0681, 0.0681, 0.0681],\n",
      "          [0.0592, 0.0364, 0.0538,  ..., 0.0800, 0.0800, 0.0800],\n",
      "          ...,\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "ì´í›„ ê³„ì‚°ëœ attention ê°’ì„ vê³¼ ê³±í•˜ì—¬ ìµœì¢… ê²°ê´ê°’ì„ ì œì‹œí•©ë‹ˆë‹¤.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê° headì˜ ê²°ê³¼ ë³‘í•©(concat)\n",
    "```python\n",
    "ê° headì˜ ê²°ê³¼ë¬¼ì„ concatí•˜ê³  ë™ì¼ ì°¨ì›(d_model)ìœ¼ë¡œ linear transformation í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì—¬ê¸°ì„œ 'd_model' ì°¨ì›ìœ¼ë¡œ linear transformation í•˜ëŠ” ì´ìœ ëŠ” transformer ëª¨ë¸ì—ì„œ ì›ë˜ì˜ ë°ì´í„°ì™€ ë”í•˜ëŠ” ì—°ì‚°(residual connection)ì´ ì¡´ì¬í•˜ì—¬ ì´ë•Œ ì°¨ì›ì„ í†µì¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "residual connection ì—°ì‚°ì€ ì•„ë˜ ì´ë¯¸ì§€ì—ì„œ Self-Attention ë¸”ë¡ ì´í›„ Addì— í•´ë‹¹í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.\n",
    "\n",
    "residual connectionì€ ì•ì„  ê°•ì˜ì—ì„œ ë°°ìš´ resnetì—ì„œ ì†Œê°œëœ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "```\n",
    "![residual](https://github.com/Pjunn/GDSC_mlstudy/blob/main/7%EC%A3%BC%EC%B0%A8/transformer_resideual_layer_norm.png?raw=true)\n",
    "ì´ë¯¸ì§€ ì¶œì²˜: https://jalammar.github.io/illustrated-transformer/ <br><br>\n",
    "-[What is Residual Connection?](https://paperswithcode.com/method/residual-connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0488,  0.0242, -0.0156,  ...,  0.0667, -0.0521, -0.0708],\n",
      "         [ 0.0072, -0.0197,  0.0192,  ...,  0.0316, -0.0945, -0.0256],\n",
      "         [-0.0033,  0.0314,  0.0124,  ...,  0.0897, -0.0901,  0.0061],\n",
      "         ...,\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260],\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260],\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260]],\n",
      "\n",
      "        [[ 0.2763, -0.1013, -0.3792,  ..., -0.0439, -0.4722,  0.0829],\n",
      "         [ 0.3047, -0.1255, -0.3194,  ...,  0.0817, -0.5480,  0.1529],\n",
      "         [ 0.2895, -0.1194, -0.3196,  ...,  0.0742, -0.5533,  0.1254],\n",
      "         ...,\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096],\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096],\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096]],\n",
      "\n",
      "        [[ 0.1128, -0.0572, -0.1686,  ...,  0.0651, -0.2529, -0.0017],\n",
      "         [ 0.0962, -0.1411, -0.1601,  ...,  0.0434, -0.2645,  0.0252],\n",
      "         [ 0.1011, -0.0842, -0.1551,  ...,  0.0522, -0.2373,  0.0153],\n",
      "         ...,\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274],\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274],\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1852,  0.0116,  0.0951,  ...,  0.0442, -0.0561, -0.0340],\n",
      "         [-0.1413,  0.0149,  0.1030,  ..., -0.0231, -0.0741, -0.0917],\n",
      "         [-0.0975,  0.0391,  0.0587,  ...,  0.0487, -0.0339, -0.0098],\n",
      "         ...,\n",
      "         [-0.1147,  0.0537, -0.0025,  ...,  0.0352, -0.0520, -0.0825],\n",
      "         [-0.1362,  0.0288,  0.0700,  ..., -0.0149, -0.0433, -0.0330],\n",
      "         [-0.1382,  0.0203,  0.0774,  ...,  0.0549, -0.0276, -0.0513]],\n",
      "\n",
      "        [[ 0.0226, -0.0298, -0.0291,  ...,  0.0142, -0.1762, -0.0125],\n",
      "         [-0.0243, -0.0504, -0.0410,  ...,  0.0158, -0.1490, -0.0319],\n",
      "         [-0.0316, -0.0088, -0.0266,  ...,  0.0352, -0.2271, -0.0102],\n",
      "         ...,\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160],\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160],\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160]],\n",
      "\n",
      "        [[ 0.0959,  0.0176, -0.1082,  ...,  0.0717, -0.2537,  0.0108],\n",
      "         [ 0.0695, -0.0352, -0.1188,  ...,  0.1041, -0.2965, -0.0210],\n",
      "         [ 0.0764, -0.0308, -0.1545,  ...,  0.0689, -0.2610,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423],\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423],\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ‘¨â€ğŸ’» <font color='green'><b>[ ì½”ë”© ]</b></font> ìœ„ì˜ ê³¼ì •ì„ ëª¨ë‘ í•©ì³ í•˜ë‚˜ì˜ Multi-head attention ëª¨ë“ˆì„ êµ¬í˜„í•´ ë´…ì‹œë‹¤.\n",
    "```python\n",
    "ğŸ™\n",
    "ì•„ë˜ì˜ Multi-head attention ëª¨ë“ˆì—ì„œ '#TODO'ë¥¼ ì±„ì›Œ ëª¨ë“ˆì„ ì™„ì„± ì‹œì¼œì£¼ì„¸ìš”.\n",
    "ìœ„ ì‹¤ìŠµì—ì„œ ë°°ìš´ ë‚´ìš©ì´ í° íŒíŠ¸ê°€ ë  ê±°ì˜ˆìš”!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, dim_model, num_heads):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    assert dim_model % num_heads == 0\n",
    "\n",
    "    self.dim_model = dim_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = dim_model // num_heads\n",
    "\n",
    "    # Q, K, V ë³€í™˜ì‹œì¼œì£¼ëŠ” ë ˆì´ì–´\n",
    "    self.w_q = nn.Linear(dim_model, dim_model)\n",
    "    self.w_k = nn.Linear(dim_model, dim_model)\n",
    "    self.w_v = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    # concatëœ ì•„ì›ƒí’‹ì„ ë³€í™˜ì‹œì¼œì£¼ëŠ” ë ˆì´ì–´\n",
    "    self.w_0 = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "  def forward(self, query, key, value):\n",
    "\n",
    "    q = self.w_q(query)\n",
    "    k = self.w_k(key)\n",
    "    v = self.w_v(value)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "    \n",
    "    attn_values = attn_values.view(attn_values.size(0), -1, self.dim_model)\n",
    "\n",
    "\n",
    "    output = self.w_0(attn_values)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "  def self_attention(self, q, k, v):\n",
    "    attn_values = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)), dim=-1)\n",
    "    attn_values = torch.matmul(attn_values, v)\n",
    "\n",
    "    return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 512\n",
    "num_heads = 8\n",
    "multihead_attn = MultiheadAttention(dim_model, num_heads)\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ğŸ‰ğŸ‰ ì„±ê³µ!!! ğŸ‰ğŸ‰ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# ì•„ë˜ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì‹¤ í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤!\n",
    "if outputs.shape == batch_emb.shape:\n",
    "    print(\"ğŸ‰ğŸ‰ğŸ‰ ì„±ê³µ!!! ğŸ‰ğŸ‰ğŸ‰\")\n",
    "else:\n",
    "    print(\"ğŸ™ ë‹¤ì‹œ ë„ì „í•´ë´ìš”!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
