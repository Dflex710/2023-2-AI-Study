{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 과제\n",
    "> 인공지능 스터디 일곱 번째 과제에 오신 것을 환영합니다! 강의를 들으면서 배운 다양한 지식들을 실습을 통해서 활용해 볼 시간을 가질 것입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
    "\n",
    "아래의 수식과 같이 계산되는 multi-head attention에서 query, key, value 벡터를 생성하기 위한 \n",
    "projection matrix $( W_{i}^{Q}, W_{i}^{K}, W_{i}^{V}​​ )$는 head 간에 sharing 된다. <br>\n",
    "***\n",
    "$ MultiHead(Q,K,V)=Concat(head_1, \\cdots, head_h)W^{O} $ (이때, $W^{O}$ 는 Output을 만들때 사용되는 가중치 행렬)<br>\n",
    "where $head_i=Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})$ (이때, $Q, K, V$ 는 입력에서 tokenize된 단어들의 임베딩 벡터 $Q = K = V$ )\n",
    "```python\n",
    "(1) 예\n",
    "(2) 아니오\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "😉\n",
    "# TODO : 정답을 적어주세요\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> Transformer\n",
    "```python\n",
    "Transformer 모델에서 각 입력 토큰들이 가진 순서를 입력하기 위해 사용하는 방법을 고르시오. \n",
    "\n",
    "(1) Positional Encoding \n",
    "(2) Encoder-Decoder attention \n",
    "(3) Layer normalization \n",
    "(4) Masked decoder self-attention \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "😉\n",
    "# TODO : 정답을 적어주세요\n",
    "1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
    "```python\n",
    "GPT-1 모델이 어떻게 다양한 자연어 처리 태스크에서 사용될 수 있는지 설명해주세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "😉\n",
    "# TODO : 정답을 적어주세요\n",
    "Transformer의 디코더 구조만을 사용, \n",
    "self-attention 매커니즘을 이용. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> GPT\n",
    "```python\n",
    "GPT-1 모델의 \"GPT\" 약자는 무엇을 의미하나요?\n",
    "\n",
    "(1) Generalized Pre-trained Transformer\n",
    "(2) Generative Pre-trained Transformer\n",
    "(3) Globalized Pre-processing Transformer\n",
    "(4) Gradient Propagation Technique\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "😉\n",
    "# TODO : 정답을 적어주세요\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ <font color='red'><b>[ 퀴즈 ]</b></font> BERT\n",
    "```python\n",
    "다음 중 BERT에 대한 설명으로 옳지 않은 것을 고르시오.\n",
    "\n",
    "(1) 학습 데이터에서 [MASK] 토큰이 선택되는 비율이 극단적으로 작은 경우, 모델 학습을 위한 비용이 증가한다. \n",
    "(2) Unidirectional model로 자연어 생성에 특화된 모델이다. \n",
    "(3) 입력 시퀀스 중 일부 마스킹된 토큰을 맞추는 masked language modeling (masked LM)을 통해 pre-training을 수행하였다. \n",
    "(4) 사전학습을 위한 [MASK] 토큰은 random하게 선택된다. \n",
    "(5) Unlabeled 데이터를 기반으로 self-supervised learning을 적용하여 사전학습한 모델이다. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "😉\n",
    "# TODO : 정답을 적어주세요\n",
    "2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 👨‍💻 <font color='green'><b>[ 실습 ]</b></font> Multi-head Attention\n",
    "```python\n",
    "이번 실습을 통해 다음 2가지를 알아볼 것입니다.\n",
    "1. Multi-head attention 및 self-attention을 구현합니다.\n",
    "2. 각 과정에서 일어나는 연산과 input/output 형태를 이해합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "🐙\n",
    "먼저 코드 실행에 필요한 패키지를 import 해봅시다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "```python\n",
    "저번 주차의 데이터와 비슷한 형태입니다.\n",
    "먼저 전체 단어 수인 vocab_size가 주어집니다.\n",
    "pad_id는 주어진 데이터의 길이를 맞춰주기 위해 패딩을 진행하게 되는데 이때 패딩을 의미하는 토큰의 id입니다.\n",
    "sample data 보면 숫자로 이루어진 것을 볼 수 있는데 이는 저희가 구성한 vocab에서 몇 번째 단어인지를 의미합니다.\n",
    "따라서 데이터의 각 요소를 단어로 이루어진 문장이라고 생각할 수 있습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "pad_id = 0\n",
    "\n",
    "data = [\n",
    "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\n",
    "  [60, 96, 51, 32, 90],\n",
    "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\n",
    "  [75, 51],\n",
    "  [66, 88, 98, 47],\n",
    "  [21, 39, 10, 64, 21],\n",
    "  [98],\n",
    "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\n",
    "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\n",
    "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "주어진 데이터의 길이를 맞춰주기 위한 padding 함수를 도입합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data):\n",
    "  max_len = len(max(data, key=len))\n",
    "  print(f\"Maximum sequence length: {max_len}\")\n",
    "\n",
    "  for i, seq in enumerate(tqdm(data)):\n",
    "    if len(seq) < max_len:\n",
    "      data[i] = seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "  return data, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data, max_len = padding(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "전처리된 데이터를 확인해 보면 잘 패딩 되었음을 확인할 수 있습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
       " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [77,\n",
       "  65,\n",
       "  51,\n",
       "  77,\n",
       "  19,\n",
       "  15,\n",
       "  35,\n",
       "  19,\n",
       "  23,\n",
       "  97,\n",
       "  50,\n",
       "  46,\n",
       "  53,\n",
       "  42,\n",
       "  45,\n",
       "  91,\n",
       "  66,\n",
       "  3,\n",
       "  43,\n",
       "  10],\n",
       " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
       " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter 세팅 및 embedding\n",
    "```python\n",
    "위 데이터를 임베딩하여 실습에 사용할 데이터를 만들어 봅시다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512  # model의 hidden size\n",
    "num_heads = 8  # multi-head에서의 head의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "# B: 배치 사이즈, L: maximum sequence length\n",
    "batch = torch.LongTensor(data)  # (B, L)\n",
    "batch_emb = embedding(batch)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.0046, -0.0178,  0.0514,  ..., -0.0254, -1.9394,  0.2982],\n",
      "         [-0.2236, -0.6979,  0.4691,  ...,  0.8568,  1.1622, -0.6842],\n",
      "         [-1.0034, -0.7161,  1.3712,  ..., -0.2181, -0.2377, -0.8508],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[ 0.4331, -0.4980, -0.2755,  ..., -0.1430,  1.2968, -0.5471],\n",
      "         [-0.5339, -0.7122,  0.6966,  ...,  1.4837, -0.7220,  0.1256],\n",
      "         [-0.6730,  0.8889,  0.8030,  ..., -1.9143, -1.0442, -1.6064],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[ 0.1244, -1.8060,  0.3670,  ...,  1.7464, -0.0483, -0.0385],\n",
      "         [ 1.6756,  0.7209, -2.4503,  ...,  0.5764,  0.0857, -1.2608],\n",
      "         [-0.8744,  0.6161, -1.1340,  ..., -0.4147, -0.9417, -0.7027],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.3047,  1.6126, -0.7530,  ..., -0.5178, -0.8630,  0.6209],\n",
      "         [ 1.2254, -0.4911,  1.8303,  ..., -0.3099, -0.8174,  1.9783],\n",
      "         [-0.6730,  0.8889,  0.8030,  ..., -1.9143, -1.0442, -1.6064],\n",
      "         ...,\n",
      "         [ 1.3433,  0.3143, -0.5930,  ..., -0.4165, -0.1450,  0.5027],\n",
      "         [-0.4853, -0.1089,  0.5043,  ...,  0.6195, -1.5236, -0.4439],\n",
      "         [-0.6199,  0.2733, -1.3323,  ...,  0.2144, -1.4687,  0.3000]],\n",
      "\n",
      "        [[-1.3036, -1.5289, -0.6917,  ...,  1.3204, -0.3155, -0.0473],\n",
      "         [ 0.9296,  0.0604, -0.7162,  ..., -1.1252, -1.0640, -0.0301],\n",
      "         [ 0.5358, -0.3090, -0.7543,  ...,  1.8795,  0.3861, -1.0702],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]],\n",
      "\n",
      "        [[-0.1491,  0.0278, -0.7670,  ..., -0.5967,  0.9102,  0.3555],\n",
      "         [ 0.9296,  0.0604, -0.7162,  ..., -1.1252, -1.0640, -0.0301],\n",
      "         [ 0.4464,  1.2593,  0.8162,  ...,  0.5223,  1.0283, -2.3301],\n",
      "         ...,\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857],\n",
      "         [ 0.1010,  1.4739,  0.2217,  ...,  0.9508,  0.8147,  0.5857]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "print(batch_emb)\n",
    "print(batch_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear transformation & 여러 head로 나누기\n",
    "```python\n",
    "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다.\n",
    "\n",
    "query, key, value를 서로 다른 linear transformation matrix로 행렬 연산을 통해 만들어 냅니다. 따라서 동일한 데이터(batch_emb)로부터 서로 다른 query, key, value를 생성할 수 있습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_q = nn.Linear(d_model, d_model)\n",
    "w_k = nn.Linear(d_model, d_model)\n",
    "w_v = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "output layer에서 사용될 행렬도 만들어 줍니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0 = nn.Linear(d_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "q = w_q(batch_emb)  # (B, L, d_model)\n",
    "k = w_k(batch_emb)  # (B, L, d_model)\n",
    "v = w_v(batch_emb)  # (B, L, d_model)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "q, k, v를 'num_head' 개의 차원으로 분할하여 여러 벡터를 만듭니다. \n",
    "실제 q, k, v 각각의 벡터 크기는 512가 아닌 64입니다. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n",
      "torch.Size([10, 20, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = q.shape[0]\n",
    "d_k = d_model // num_heads # q, k, v 벡터 사이즈\n",
    "\n",
    "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "8개의 head에 필요한 q, k, v가 만들어졌습니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n",
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled dot-product self-attention 구현\n",
    "```python\n",
    "각 head에서 실행되는 self-attention 과정을 살펴봅시다.\n",
    "\n",
    "q, k 벡터의 내적 연산 이후에 d_k의 제곱근으로 나눠줍니다.\n",
    "이는 q와 k를 구성하는 요소의 평균과 분산을 내적의 결괏값에 대해서도 유지시켜주기 위함입니다.\n",
    "\n",
    "이후 계산된 각 행에 대해서 softmax 연산을 통해서 각 요소의 합을 1로 만들어줍니다.\n",
    "```\n",
    "- [Scaled Dot-Product Attention 참고](https://paperswithcode.com/method/scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0629, 0.0237, 0.0601,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0625, 0.0487, 0.0453,  ..., 0.0663, 0.0663, 0.0663],\n",
      "          [0.0517, 0.0301, 0.0620,  ..., 0.0318, 0.0318, 0.0318],\n",
      "          ...,\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933],\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933],\n",
      "          [0.0370, 0.0353, 0.0606,  ..., 0.0933, 0.0933, 0.0933]],\n",
      "\n",
      "         [[0.0580, 0.0319, 0.0398,  ..., 0.0736, 0.0736, 0.0736],\n",
      "          [0.0963, 0.0371, 0.1088,  ..., 0.0319, 0.0319, 0.0319],\n",
      "          [0.0523, 0.0234, 0.0327,  ..., 0.0612, 0.0612, 0.0612],\n",
      "          ...,\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363],\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363],\n",
      "          [0.0294, 0.0835, 0.0319,  ..., 0.0363, 0.0363, 0.0363]],\n",
      "\n",
      "         [[0.0374, 0.0362, 0.0524,  ..., 0.0588, 0.0588, 0.0588],\n",
      "          [0.0536, 0.0321, 0.0599,  ..., 0.0679, 0.0679, 0.0679],\n",
      "          [0.0631, 0.0438, 0.0243,  ..., 0.0518, 0.0518, 0.0518],\n",
      "          ...,\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644],\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644],\n",
      "          [0.0393, 0.0367, 0.0614,  ..., 0.0644, 0.0644, 0.0644]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0488, 0.0509, 0.0478,  ..., 0.0513, 0.0513, 0.0513],\n",
      "          [0.0396, 0.0485, 0.0265,  ..., 0.0579, 0.0579, 0.0579],\n",
      "          [0.0731, 0.0345, 0.0879,  ..., 0.0409, 0.0409, 0.0409],\n",
      "          ...,\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351],\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351],\n",
      "          [0.0443, 0.0897, 0.0396,  ..., 0.0351, 0.0351, 0.0351]],\n",
      "\n",
      "         [[0.0205, 0.0379, 0.0399,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0738, 0.0668, 0.0907,  ..., 0.0484, 0.0484, 0.0484],\n",
      "          [0.0361, 0.0620, 0.0609,  ..., 0.0434, 0.0434, 0.0434],\n",
      "          ...,\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051],\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051],\n",
      "          [0.0249, 0.0317, 0.0311,  ..., 0.1051, 0.1051, 0.1051]],\n",
      "\n",
      "         [[0.0627, 0.0410, 0.0506,  ..., 0.0316, 0.0316, 0.0316],\n",
      "          [0.0694, 0.0395, 0.0463,  ..., 0.0397, 0.0397, 0.0397],\n",
      "          [0.0560, 0.0404, 0.0482,  ..., 0.0494, 0.0494, 0.0494],\n",
      "          ...,\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0393, 0.0651, 0.0481,  ..., 0.0456, 0.0456, 0.0456]]],\n",
      "\n",
      "\n",
      "        [[[0.0340, 0.0332, 0.0250,  ..., 0.0574, 0.0574, 0.0574],\n",
      "          [0.0513, 0.0525, 0.0365,  ..., 0.0469, 0.0469, 0.0469],\n",
      "          [0.0415, 0.0624, 0.0690,  ..., 0.0488, 0.0488, 0.0488],\n",
      "          ...,\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0454, 0.0172, 0.0338,  ..., 0.0572, 0.0572, 0.0572]],\n",
      "\n",
      "         [[0.0517, 0.0980, 0.0389,  ..., 0.0456, 0.0456, 0.0456],\n",
      "          [0.0375, 0.0275, 0.0217,  ..., 0.0559, 0.0559, 0.0559],\n",
      "          [0.0383, 0.0633, 0.0467,  ..., 0.0524, 0.0524, 0.0524],\n",
      "          ...,\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0289, 0.1042, 0.0772,  ..., 0.0455, 0.0455, 0.0455]],\n",
      "\n",
      "         [[0.0161, 0.0252, 0.0438,  ..., 0.0567, 0.0567, 0.0567],\n",
      "          [0.0759, 0.0470, 0.0684,  ..., 0.0487, 0.0487, 0.0487],\n",
      "          [0.0322, 0.0915, 0.0625,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          ...,\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0445, 0.0224, 0.0398,  ..., 0.0545, 0.0545, 0.0545]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0554, 0.0346, 0.0785,  ..., 0.0492, 0.0492, 0.0492],\n",
      "          [0.0289, 0.0522, 0.0288,  ..., 0.0550, 0.0550, 0.0550],\n",
      "          [0.0485, 0.0581, 0.0709,  ..., 0.0488, 0.0488, 0.0488],\n",
      "          ...,\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0705, 0.1053, 0.0533,  ..., 0.0418, 0.0418, 0.0418]],\n",
      "\n",
      "         [[0.0623, 0.0978, 0.0710,  ..., 0.0399, 0.0399, 0.0399],\n",
      "          [0.0659, 0.0554, 0.0182,  ..., 0.0497, 0.0497, 0.0497],\n",
      "          [0.0357, 0.0322, 0.0543,  ..., 0.0540, 0.0540, 0.0540],\n",
      "          ...,\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602],\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602],\n",
      "          [0.0195, 0.0213, 0.0263,  ..., 0.0602, 0.0602, 0.0602]],\n",
      "\n",
      "         [[0.0891, 0.0857, 0.0783,  ..., 0.0425, 0.0425, 0.0425],\n",
      "          [0.0467, 0.0496, 0.0789,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0303, 0.0442, 0.0380,  ..., 0.0522, 0.0522, 0.0522],\n",
      "          ...,\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489],\n",
      "          [0.0839, 0.0478, 0.0455,  ..., 0.0489, 0.0489, 0.0489]]],\n",
      "\n",
      "\n",
      "        [[[0.0501, 0.0406, 0.0330,  ..., 0.0519, 0.0519, 0.0519],\n",
      "          [0.0455, 0.0791, 0.0343,  ..., 0.0554, 0.0554, 0.0554],\n",
      "          [0.0574, 0.0697, 0.0507,  ..., 0.0475, 0.0475, 0.0475],\n",
      "          ...,\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704],\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704],\n",
      "          [0.0217, 0.0409, 0.0330,  ..., 0.0704, 0.0704, 0.0704]],\n",
      "\n",
      "         [[0.0152, 0.0379, 0.0888,  ..., 0.0533, 0.0533, 0.0533],\n",
      "          [0.0202, 0.0375, 0.0333,  ..., 0.0606, 0.0606, 0.0606],\n",
      "          [0.0370, 0.0579, 0.0524,  ..., 0.0460, 0.0460, 0.0460],\n",
      "          ...,\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418],\n",
      "          [0.0567, 0.0434, 0.0398,  ..., 0.0418, 0.0418, 0.0418]],\n",
      "\n",
      "         [[0.0367, 0.0411, 0.0360,  ..., 0.0719, 0.0719, 0.0719],\n",
      "          [0.0695, 0.0444, 0.0977,  ..., 0.0432, 0.0432, 0.0432],\n",
      "          [0.0428, 0.0307, 0.0533,  ..., 0.0648, 0.0648, 0.0648],\n",
      "          ...,\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531],\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531],\n",
      "          [0.0675, 0.0425, 0.0502,  ..., 0.0531, 0.0531, 0.0531]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0797, 0.0861, 0.0643,  ..., 0.0410, 0.0410, 0.0410],\n",
      "          [0.0481, 0.0766, 0.0404,  ..., 0.0572, 0.0572, 0.0572],\n",
      "          [0.0571, 0.0580, 0.0419,  ..., 0.0557, 0.0557, 0.0557],\n",
      "          ...,\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          [0.1010, 0.0830, 0.0263,  ..., 0.0458, 0.0458, 0.0458]],\n",
      "\n",
      "         [[0.0623, 0.0289, 0.0428,  ..., 0.0575, 0.0575, 0.0575],\n",
      "          [0.1106, 0.0613, 0.0390,  ..., 0.0374, 0.0374, 0.0374],\n",
      "          [0.0342, 0.0826, 0.0458,  ..., 0.0451, 0.0451, 0.0451],\n",
      "          ...,\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732],\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732],\n",
      "          [0.0157, 0.0227, 0.0198,  ..., 0.0732, 0.0732, 0.0732]],\n",
      "\n",
      "         [[0.1068, 0.0316, 0.0453,  ..., 0.0545, 0.0545, 0.0545],\n",
      "          [0.0559, 0.0331, 0.0520,  ..., 0.0485, 0.0485, 0.0485],\n",
      "          [0.0578, 0.0602, 0.0643,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          ...,\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512],\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512],\n",
      "          [0.0611, 0.0493, 0.0351,  ..., 0.0512, 0.0512, 0.0512]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0540, 0.0292, 0.0635,  ..., 0.0614, 0.0748, 0.0903],\n",
      "          [0.0387, 0.0318, 0.0448,  ..., 0.0503, 0.0256, 0.0408],\n",
      "          [0.0491, 0.0541, 0.0729,  ..., 0.0268, 0.0444, 0.0708],\n",
      "          ...,\n",
      "          [0.0582, 0.0522, 0.0326,  ..., 0.0538, 0.0440, 0.0509],\n",
      "          [0.0284, 0.0537, 0.0520,  ..., 0.0431, 0.0633, 0.0451],\n",
      "          [0.0441, 0.0635, 0.0324,  ..., 0.0536, 0.0577, 0.0481]],\n",
      "\n",
      "         [[0.0374, 0.0564, 0.0266,  ..., 0.0421, 0.0466, 0.1090],\n",
      "          [0.0520, 0.0423, 0.0377,  ..., 0.0448, 0.0492, 0.0338],\n",
      "          [0.0509, 0.0425, 0.0595,  ..., 0.0770, 0.0455, 0.0954],\n",
      "          ...,\n",
      "          [0.0429, 0.0212, 0.0318,  ..., 0.0383, 0.0559, 0.0555],\n",
      "          [0.0466, 0.0496, 0.0412,  ..., 0.0497, 0.0364, 0.0462],\n",
      "          [0.0474, 0.0542, 0.0637,  ..., 0.0731, 0.0510, 0.0398]],\n",
      "\n",
      "         [[0.0276, 0.0645, 0.0320,  ..., 0.0632, 0.0341, 0.0871],\n",
      "          [0.0777, 0.0481, 0.0545,  ..., 0.0402, 0.0395, 0.0585],\n",
      "          [0.0565, 0.0505, 0.0651,  ..., 0.0322, 0.0609, 0.0208],\n",
      "          ...,\n",
      "          [0.0498, 0.0659, 0.0672,  ..., 0.0299, 0.0586, 0.0997],\n",
      "          [0.0562, 0.0501, 0.0690,  ..., 0.0466, 0.0342, 0.0540],\n",
      "          [0.0252, 0.0628, 0.0279,  ..., 0.0563, 0.0433, 0.0385]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0508, 0.0720, 0.0604,  ..., 0.0444, 0.0448, 0.0553],\n",
      "          [0.0316, 0.0658, 0.0528,  ..., 0.0611, 0.0443, 0.0478],\n",
      "          [0.0593, 0.0343, 0.0771,  ..., 0.0591, 0.0399, 0.0568],\n",
      "          ...,\n",
      "          [0.0600, 0.0447, 0.0638,  ..., 0.0588, 0.0335, 0.0438],\n",
      "          [0.0185, 0.0296, 0.0535,  ..., 0.0467, 0.0475, 0.0717],\n",
      "          [0.0825, 0.0294, 0.0827,  ..., 0.0184, 0.0390, 0.0930]],\n",
      "\n",
      "         [[0.0334, 0.0249, 0.0567,  ..., 0.0545, 0.0429, 0.0687],\n",
      "          [0.0447, 0.0457, 0.0252,  ..., 0.0561, 0.0541, 0.0429],\n",
      "          [0.0408, 0.0446, 0.0624,  ..., 0.0522, 0.0270, 0.0545],\n",
      "          ...,\n",
      "          [0.0473, 0.0511, 0.0536,  ..., 0.0507, 0.0457, 0.0356],\n",
      "          [0.0552, 0.0394, 0.0609,  ..., 0.0383, 0.0917, 0.0337],\n",
      "          [0.0435, 0.0333, 0.0637,  ..., 0.0375, 0.0780, 0.0404]],\n",
      "\n",
      "         [[0.0441, 0.0548, 0.0282,  ..., 0.0886, 0.0459, 0.0957],\n",
      "          [0.0788, 0.0607, 0.0300,  ..., 0.0868, 0.0515, 0.0336],\n",
      "          [0.0385, 0.0325, 0.0361,  ..., 0.0602, 0.0608, 0.0418],\n",
      "          ...,\n",
      "          [0.0405, 0.0602, 0.0286,  ..., 0.0277, 0.0511, 0.1017],\n",
      "          [0.0614, 0.0429, 0.0553,  ..., 0.0204, 0.0496, 0.0608],\n",
      "          [0.0378, 0.0835, 0.0355,  ..., 0.0626, 0.0542, 0.0361]]],\n",
      "\n",
      "\n",
      "        [[[0.0561, 0.0734, 0.0532,  ..., 0.0455, 0.0455, 0.0455],\n",
      "          [0.0392, 0.0535, 0.0559,  ..., 0.0392, 0.0392, 0.0392],\n",
      "          [0.0448, 0.0553, 0.0715,  ..., 0.0497, 0.0497, 0.0497],\n",
      "          ...,\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846],\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846],\n",
      "          [0.0302, 0.0446, 0.0336,  ..., 0.0846, 0.0846, 0.0846]],\n",
      "\n",
      "         [[0.0490, 0.0404, 0.0389,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0639, 0.0679, 0.0347,  ..., 0.0486, 0.0486, 0.0486],\n",
      "          [0.0395, 0.0394, 0.0407,  ..., 0.0613, 0.0613, 0.0613],\n",
      "          ...,\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393],\n",
      "          [0.0560, 0.0540, 0.0738,  ..., 0.0393, 0.0393, 0.0393]],\n",
      "\n",
      "         [[0.0665, 0.0689, 0.0335,  ..., 0.0276, 0.0276, 0.0276],\n",
      "          [0.0324, 0.0537, 0.0563,  ..., 0.0713, 0.0713, 0.0713],\n",
      "          [0.0433, 0.0589, 0.0424,  ..., 0.0490, 0.0490, 0.0490],\n",
      "          ...,\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548],\n",
      "          [0.0306, 0.0631, 0.0531,  ..., 0.0548, 0.0548, 0.0548]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0486, 0.0375, 0.0293,  ..., 0.0331, 0.0331, 0.0331],\n",
      "          [0.0474, 0.0506, 0.0850,  ..., 0.0317, 0.0317, 0.0317],\n",
      "          [0.0737, 0.0350, 0.0440,  ..., 0.0568, 0.0568, 0.0568],\n",
      "          ...,\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339],\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339],\n",
      "          [0.0705, 0.0307, 0.0235,  ..., 0.0339, 0.0339, 0.0339]],\n",
      "\n",
      "         [[0.0635, 0.0552, 0.0335,  ..., 0.0479, 0.0479, 0.0479],\n",
      "          [0.0727, 0.0647, 0.0462,  ..., 0.0422, 0.0422, 0.0422],\n",
      "          [0.0297, 0.0630, 0.0395,  ..., 0.0458, 0.0458, 0.0458],\n",
      "          ...,\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020],\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020],\n",
      "          [0.0307, 0.0820, 0.0336,  ..., 0.1020, 0.1020, 0.1020]],\n",
      "\n",
      "         [[0.0329, 0.0462, 0.0435,  ..., 0.0500, 0.0500, 0.0500],\n",
      "          [0.0495, 0.0337, 0.0394,  ..., 0.0809, 0.0809, 0.0809],\n",
      "          [0.0296, 0.0239, 0.0404,  ..., 0.0298, 0.0298, 0.0298],\n",
      "          ...,\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450],\n",
      "          [0.0321, 0.0426, 0.0515,  ..., 0.0450, 0.0450, 0.0450]]],\n",
      "\n",
      "\n",
      "        [[[0.0506, 0.0462, 0.0248,  ..., 0.0879, 0.0879, 0.0879],\n",
      "          [0.0429, 0.0557, 0.0568,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0598, 0.0408, 0.0215,  ..., 0.0632, 0.0632, 0.0632],\n",
      "          ...,\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816],\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816],\n",
      "          [0.0326, 0.0430, 0.0222,  ..., 0.0816, 0.0816, 0.0816]],\n",
      "\n",
      "         [[0.0441, 0.0474, 0.0594,  ..., 0.0521, 0.0521, 0.0521],\n",
      "          [0.0569, 0.0705, 0.0384,  ..., 0.0504, 0.0504, 0.0504],\n",
      "          [0.0389, 0.0494, 0.0672,  ..., 0.0421, 0.0421, 0.0421],\n",
      "          ...,\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412],\n",
      "          [0.0504, 0.0566, 0.0668,  ..., 0.0412, 0.0412, 0.0412]],\n",
      "\n",
      "         [[0.0835, 0.0616, 0.0610,  ..., 0.0327, 0.0327, 0.0327],\n",
      "          [0.0404, 0.0468, 0.0411,  ..., 0.0622, 0.0622, 0.0622],\n",
      "          [0.0649, 0.0360, 0.0775,  ..., 0.0501, 0.0501, 0.0501],\n",
      "          ...,\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547],\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547],\n",
      "          [0.0458, 0.0630, 0.0335,  ..., 0.0547, 0.0547, 0.0547]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0595, 0.0512, 0.0331,  ..., 0.0511, 0.0511, 0.0511],\n",
      "          [0.0579, 0.0606, 0.0493,  ..., 0.0379, 0.0379, 0.0379],\n",
      "          [0.0464, 0.0409, 0.0469,  ..., 0.0424, 0.0424, 0.0424],\n",
      "          ...,\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408],\n",
      "          [0.0346, 0.0370, 0.0540,  ..., 0.0408, 0.0408, 0.0408]],\n",
      "\n",
      "         [[0.0203, 0.0627, 0.0445,  ..., 0.0562, 0.0562, 0.0562],\n",
      "          [0.0458, 0.0660, 0.0727,  ..., 0.0430, 0.0430, 0.0430],\n",
      "          [0.0415, 0.0697, 0.0730,  ..., 0.0407, 0.0407, 0.0407],\n",
      "          ...,\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837],\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837],\n",
      "          [0.0287, 0.0673, 0.0306,  ..., 0.0837, 0.0837, 0.0837]],\n",
      "\n",
      "         [[0.0542, 0.0644, 0.0629,  ..., 0.0603, 0.0603, 0.0603],\n",
      "          [0.0435, 0.0284, 0.0353,  ..., 0.0681, 0.0681, 0.0681],\n",
      "          [0.0592, 0.0364, 0.0538,  ..., 0.0800, 0.0800, 0.0800],\n",
      "          ...,\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453],\n",
      "          [0.0472, 0.0429, 0.0624,  ..., 0.0453, 0.0453, 0.0453]]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([10, 8, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\n",
    "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\n",
    "\n",
    "print(attn_dists)\n",
    "print(attn_dists.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "이후 계산된 attention 값을 v과 곱하여 최종 결괏값을 제시합니다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 8, 20, 64])\n"
     ]
    }
   ],
   "source": [
    "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 head의 결과 병합(concat)\n",
    "```python\n",
    "각 head의 결과물을 concat하고 동일 차원(d_model)으로 linear transformation 합니다. \n",
    "\n",
    "여기서 'd_model' 차원으로 linear transformation 하는 이유는 transformer 모델에서 원래의 데이터와 더하는 연산(residual connection)이 존재하여 이때 차원을 통일해야 하기 때문입니다.\n",
    "\n",
    "residual connection 연산은 아래 이미지에서 Self-Attention 블록 이후 Add에 해당하는 연산입니다.\n",
    "\n",
    "residual connection은 앞선 강의에서 배운 resnet에서 소개된 기술입니다.\n",
    "```\n",
    "![residual](https://github.com/Pjunn/GDSC_mlstudy/blob/main/7%EC%A3%BC%EC%B0%A8/transformer_resideual_layer_norm.png?raw=true)\n",
    "이미지 출처: https://jalammar.github.io/illustrated-transformer/ <br><br>\n",
    "-[What is Residual Connection?](https://paperswithcode.com/method/residual-connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\n",
    "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\n",
    "\n",
    "print(attn_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0488,  0.0242, -0.0156,  ...,  0.0667, -0.0521, -0.0708],\n",
      "         [ 0.0072, -0.0197,  0.0192,  ...,  0.0316, -0.0945, -0.0256],\n",
      "         [-0.0033,  0.0314,  0.0124,  ...,  0.0897, -0.0901,  0.0061],\n",
      "         ...,\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260],\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260],\n",
      "         [ 0.0491,  0.0538,  0.0265,  ...,  0.0068, -0.1068, -0.0260]],\n",
      "\n",
      "        [[ 0.2763, -0.1013, -0.3792,  ..., -0.0439, -0.4722,  0.0829],\n",
      "         [ 0.3047, -0.1255, -0.3194,  ...,  0.0817, -0.5480,  0.1529],\n",
      "         [ 0.2895, -0.1194, -0.3196,  ...,  0.0742, -0.5533,  0.1254],\n",
      "         ...,\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096],\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096],\n",
      "         [ 0.2891, -0.1185, -0.2968,  ...,  0.0525, -0.5143,  0.1096]],\n",
      "\n",
      "        [[ 0.1128, -0.0572, -0.1686,  ...,  0.0651, -0.2529, -0.0017],\n",
      "         [ 0.0962, -0.1411, -0.1601,  ...,  0.0434, -0.2645,  0.0252],\n",
      "         [ 0.1011, -0.0842, -0.1551,  ...,  0.0522, -0.2373,  0.0153],\n",
      "         ...,\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274],\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274],\n",
      "         [ 0.1191, -0.0644, -0.1377,  ...,  0.0282, -0.2757,  0.0274]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1852,  0.0116,  0.0951,  ...,  0.0442, -0.0561, -0.0340],\n",
      "         [-0.1413,  0.0149,  0.1030,  ..., -0.0231, -0.0741, -0.0917],\n",
      "         [-0.0975,  0.0391,  0.0587,  ...,  0.0487, -0.0339, -0.0098],\n",
      "         ...,\n",
      "         [-0.1147,  0.0537, -0.0025,  ...,  0.0352, -0.0520, -0.0825],\n",
      "         [-0.1362,  0.0288,  0.0700,  ..., -0.0149, -0.0433, -0.0330],\n",
      "         [-0.1382,  0.0203,  0.0774,  ...,  0.0549, -0.0276, -0.0513]],\n",
      "\n",
      "        [[ 0.0226, -0.0298, -0.0291,  ...,  0.0142, -0.1762, -0.0125],\n",
      "         [-0.0243, -0.0504, -0.0410,  ...,  0.0158, -0.1490, -0.0319],\n",
      "         [-0.0316, -0.0088, -0.0266,  ...,  0.0352, -0.2271, -0.0102],\n",
      "         ...,\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160],\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160],\n",
      "         [ 0.0367, -0.0372, -0.0594,  ..., -0.0095, -0.1673, -0.0160]],\n",
      "\n",
      "        [[ 0.0959,  0.0176, -0.1082,  ...,  0.0717, -0.2537,  0.0108],\n",
      "         [ 0.0695, -0.0352, -0.1188,  ...,  0.1041, -0.2965, -0.0210],\n",
      "         [ 0.0764, -0.0308, -0.1545,  ...,  0.0689, -0.2610,  0.0015],\n",
      "         ...,\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423],\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423],\n",
      "         [ 0.0365,  0.0266, -0.0715,  ...,  0.0686, -0.3042, -0.0423]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "outputs = w_0(attn_values)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 👨‍💻 <font color='green'><b>[ 코딩 ]</b></font> 위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현해 봅시다.\n",
    "```python\n",
    "🐙\n",
    "아래의 Multi-head attention 모듈에서 '#TODO'를 채워 모듈을 완성 시켜주세요.\n",
    "위 실습에서 배운 내용이 큰 힌트가 될 거예요!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "  def __init__(self, dim_model, num_heads):\n",
    "    super(MultiheadAttention, self).__init__()\n",
    "\n",
    "    assert dim_model % num_heads == 0\n",
    "\n",
    "    self.dim_model = dim_model\n",
    "    self.num_heads = num_heads\n",
    "    self.d_k = dim_model // num_heads\n",
    "\n",
    "    # Q, K, V 변환시켜주는 레이어\n",
    "    self.w_q = nn.Linear(dim_model, dim_model)\n",
    "    self.w_k = nn.Linear(dim_model, dim_model)\n",
    "    self.w_v = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    # concat된 아웃풋을 변환시켜주는 레이어\n",
    "    self.w_0 = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "  def forward(self, query, key, value):\n",
    "\n",
    "    q = self.w_q(query)\n",
    "    k = self.w_k(key)\n",
    "    v = self.w_v(value)\n",
    "\n",
    "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\n",
    "    \n",
    "    attn_values = attn_values.view(attn_values.size(0), -1, self.dim_model)\n",
    "\n",
    "\n",
    "    output = self.w_0(attn_values)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "  def self_attention(self, q, k, v):\n",
    "    attn_values = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)), dim=-1)\n",
    "    attn_values = torch.matmul(attn_values, v)\n",
    "\n",
    "    return attn_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 512\n",
    "num_heads = 8\n",
    "multihead_attn = MultiheadAttention(dim_model, num_heads)\n",
    "\n",
    "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
     ]
    }
   ],
   "source": [
    "# 아래 코드는 수정하실 필요가 없습니다!\n",
    "if outputs.shape == batch_emb.shape:\n",
    "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
    "else:\n",
    "    print(\"🐙 다시 도전해봐요!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
